{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24ae79c-3a01-43a9-bc5d-a8bc308247b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9d568a-1d5c-4d04-ba74-18860d60cfb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os, pathlib, shutil, platform\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "from prophet import Prophet\n",
    "\n",
    "from prophet.diagnostics import cross_validation\n",
    "from prophet.diagnostics import performance_metrics\n",
    "\n",
    "from prophet.plot import plot_plotly, plot_components_plotly\n",
    "from prophet.plot import plot_cross_validation_metric\n",
    "\n",
    "# import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import humanize\n",
    "from datetime import date, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d88026-5782-4bec-aad2-573d5f752b9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    " \n",
    "plt.rcParams['figure.figsize']=(20,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba68809-244e-4e45-a3f9-cb56bb942568",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## rewrote prophet's make future dataframe cuz it doesnt handle very well only monthly data; \n",
    "# fc_model.make_future_dataframe(periods=10, freq='m') ##compare my_make_future_dataframe with the one provided by Prophet\n",
    "def my_make_future_dataframe(df, periods):\n",
    "    last_date=df['ds'].max()\n",
    "    complete_df = df.append(pd.DataFrame([last_date + relativedelta(months = i + 1) for i in range(periods)],\n",
    "                                              columns =['ds']), ignore_index=True, sort=True)\n",
    "    return complete_df\n",
    "\n",
    "def forecast_future(future_samples_count, df, growth = 'linear'):\n",
    "    model = Prophet(growth=growth)\n",
    "    model.fit(df)\n",
    "    \n",
    "    future = my_make_future_dataframe(df, periods=future_samples_count)\n",
    "\n",
    "    forecast = model.predict(future)\n",
    "    return forecast, model\n",
    "\n",
    "def forecast_in_sample(hold_out_samples_count, df, growth = 'linear'):\n",
    "    train_data = df.drop(df.index[-hold_out_samples_count:])\n",
    "    print(train_data.head(4), train_data.tail(4))\n",
    "    print(train_data.shape)\n",
    "    \n",
    "    model = Prophet(growth=growth)\n",
    "    model.fit(train_data)\n",
    "    \n",
    "    future = df[['ds']].reset_index()                         # predicts for all ds values\n",
    "    forecast = model.predict(future)\n",
    "    return forecast, model\n",
    "\n",
    "def forecasted_percentiles(fc_model, input_df, percentiles):     \n",
    "    forecasted_samples = fc_model.predictive_samples(input_df)\n",
    "    forecasted_stats=pd.DataFrame(data=np.transpose(np.percentile(forecasted_samples['yhat'], percentiles, axis=1 )) #made a change, it said 'yhat' before 'Predicted'\n",
    "             ,  columns = ['pct_'+str(x) for x in percentiles])\n",
    "    forecasted_stats.insert(loc=0, column='Predicted', value=input_df['yhat'])\n",
    "    forecasted_stats.insert(loc=0, column='ds', value=input_df['ds'])\n",
    "    return forecasted_stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c8d680-a780-4a06-aa79-20d95fd10e01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_datasets(data_path, years_list, analysis_type_list):\n",
    "    \"\"\"\n",
    "    Concatanates a list of datasets into one dataframe, and also does yearly agg\n",
    "\n",
    "    Returns:\n",
    "    Concatanated list as a dataframe\n",
    "    \"\"\"\n",
    "    #concat\n",
    "    full_path_list = [data_path+str(crt_year) + '_analysis'+ str(analysis_type) +'_' + str(regional_flag)+'_'+str(aircraft_flag) + '.csv' \n",
    "                      for crt_year in years_list \n",
    "                      for analysis_type in analysis_type_list]\n",
    "    print(full_path_list[0])\n",
    "    all_datasets= pd.concat([pd.read_csv(str(crt_file_name)) for crt_file_name in full_path_list], keys=years_list).reset_index()\n",
    "    \n",
    "    #yearly agg\n",
    "    all_datasets.drop(columns=['level_1'],axis=1,inplace=True)\n",
    "    all_datasets = all_datasets.rename(columns = {'level_0':'year'})\n",
    "    all_datasets = all_datasets.groupby(['year', 'origin', 'dest'], as_index=True, group_keys=True)['y', 'num flights' ].agg(['sum','count'])\n",
    "    all_datasets.reset_index(inplace=True)\n",
    "    all_datasets.columns= ['_'.join(col) for col in all_datasets.columns.values]\n",
    "    all_datasets.drop(['origin_', 'dest_', 'y_count', 'num flights_count','num flights_sum'], axis=1, inplace = True)\n",
    "    all_datasets.rename(columns={'y_sum': 'y', 'year_':'ds' }, inplace = True)\n",
    "\n",
    "# my_testing[\"year\"] = my_testing['ds'].dt.year #tagged this cuz level_0 column takes care of it\n",
    "\n",
    "    return all_datasets    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b5765d-b847-449f-a6ac-ecf315d9840f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# years_list = [yr for yr in range (1990,2024)]\n",
    "# these are the years lists we use when regional_flag = regional and aircraft_flag = nonjets\n",
    "# years_list_s1 = [yr for yr in range (2004,2024)]\n",
    "# years_list_s5 = [yr for yr in range (2002,2024)]\n",
    "# years_list_s2 = [yr for yr in range (2002,2020)]\n",
    "# years_list_s3 = [yr for yr in range (2002,2020)]\n",
    "# years_list_s4 = [yr for yr in range (2002,2020)]\n",
    "\n",
    "# these are the years lists we use when regional_flag = regional and aircraft_flag = jetsandnonjets\n",
    "# years_list_s1 = [yr for yr in range (2004,2024)]\n",
    "# years_list_s5 = [yr for yr in range (2002,2024)]\n",
    "# years_list_s2 = [yr for yr in range (2002,2024)]\n",
    "# years_list_s3 = [yr for yr in range (2002,2024)]\n",
    "# years_list_s4 = [yr for yr in range (2002,2020)]\n",
    "\n",
    "# these are the years lists we use when regional_flag = regional and aircraft_flag = jets\n",
    "years_list_s1 = [yr for yr in range (2004,2024)]\n",
    "years_list_s2 = [yr for yr in range (2002,2024)]\n",
    "years_list_s3 = [yr for yr in range (2002,2024)]\n",
    "\n",
    "\n",
    "\n",
    "data_path='./../../../data/paav_cargo/agg_data/freight_aggregation__' \n",
    "\n",
    "analysis_type_list = [analysis_type for analysis_type in range(11)]\n",
    "# regional_flag = 'alldistance' # , 'regional']\n",
    "# aircraft_flag = 'jetsandnonjets' # 'nonjets']\n",
    "regional_flag = 'regional' # , 'alldistance']\n",
    "# aircraft_flag = 'nonjets' # 'jetsandnonjets']\n",
    "aircraft_flag = 'jets'\n",
    "\n",
    "# analysis_type_list = [0]\n",
    "# preprocessed_data_s1 = load_datasets(data_path, years_list_s1, analysis_type_list)\n",
    "\n",
    "# analysis_type_list = [7]\n",
    "# preprocessed_data_s2 = load_datasets(data_path, years_list_s2, analysis_type_list)\n",
    "\n",
    "analysis_type_list = [8]\n",
    "preprocessed_data_s3 = load_datasets(data_path, years_list_s3, analysis_type_list)\n",
    "\n",
    "# analysis_type_list = [9]\n",
    "# preprocessed_data_s4 = load_datasets(data_path, years_list_s4, analysis_type_list)\n",
    "\n",
    "# analysis_type_list = [10]\n",
    "# preprocessed_data_s5 = load_datasets(data_path, years_list_s5, analysis_type_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ccb3ca-0e9d-4481-8ee9-38573c5843d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_data_s3.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701b36c8-1f68-44e2-a172-f84694972218",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_data_s3.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f8530d-540a-47cb-a11a-623fff0ede3c",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdf5a6d-cfcc-40be-a340-66f087107139",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# m_s1 = Prophet()\n",
    "# m_s1.fit(preprocessed_data_s1[preprocessed_data_s1['ds'] < 2023]) \n",
    "#less than 2023 bc 2023 doesnt have all months in and gives us huge errors which are not rep of prophets performance\n",
    "# fit fxn creates coeffs for model but those are not passed to cross_validation fxn below. when u fit a df to model, the model will remember/ record the ds and y columns and \n",
    "# that's the purpose here. the ds and y memory is passed to cross val fxn. idk why they ddint just ask for ds and y column from a df as oppsoed to fitting like we do here\n",
    "# which is kind of useless.\n",
    "\n",
    "m_s3 = Prophet()\n",
    "m_s3.fit(preprocessed_data_s3[preprocessed_data_s3['ds'] < 2023]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c20fc9-a220-489f-b2ee-7aa8f1913680",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_time_s1 = f'{365*8} days' # yr 2012; 8 years is the largest initial training period i can make bc i have less data now (2004+ as opposed to 1990+)\n",
    "initial_time = f'{365*10} days'  \n",
    "initial_time_s4 =f'{365*7} days'\n",
    "#yr 2009 for s5 (because its data only begins in october 2002), s4 (because its data only begins in october 2002), s2 (because its data only begins in october 2002)\n",
    "#yr 2008 for s3 (bc its data only begins march 2001 but bc theres zero freight in that one flight, it actually begins in october 2002)\n",
    "initial_time\n",
    "# cross_validation_results_s1 = cross_validation(m_s1, initial=initial_time_s1, period='365 days', horizon = '3650 days', parallel=\"processes\")\n",
    "cross_validation_results_s3= cross_validation(m_s3, initial=initial_time, period=f'{365*1} days', horizon = '3650 days', parallel=\"processes\")\n",
    "# cross_validation_results_s4= cross_validation(m_s4, initial=initial_time_s4, period=f'{365*1} days', horizon = '3650 days', parallel=\"processes\")\n",
    "\n",
    "cross_validation_results_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb67960-1c1b-463c-a91f-7f688fb4536d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cross_validation_results_s3.rename(columns={'cutoff': 'real_cutoff'},inplace=True) \n",
    "cross_validation_results_s3['cutoff'] = np.NaN                                     \n",
    "for index, row in cross_validation_results_s3.iterrows():\n",
    "    cross_validation_results_s3.at[index, 'cutoff'] = row['ds'] - timedelta(days=365*(((index)%10)+1))\n",
    "                                      \n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#     display(cross_validation_results)\n",
    "cross_validation_results_s3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d9ebbe-395d-44a6-9449-3895e7bb7653",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cross_validation_results[\"sanity check diff1\"] = cross_validation_results[\"real_cutoff\"] - cross_validation_results[\"cutoff\"]\n",
    "cross_validation_results_s3[\"sanity check diff\"] = cross_validation_results_s3['ds'] - cross_validation_results_s3[\"cutoff\"]\n",
    "\n",
    "\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#     display(cross_validation_results.sort_values(by='sanity check diff', ascending = False, inplace = False))\n",
    "# cross_validation_results.sort_values(by='sanity check diff1', ascending = False, inplace = False).head(12)\n",
    "cross_validation_results_s3.head(2)\n",
    "cross_validation_results_s3.tail(2)\n",
    "(cross_validation_results_s3['sanity check diff']/np.timedelta64(1,'Y')).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29428550-6272-421b-8d51-31f6120de994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(cross_validation_results_s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce1ac2a-6c59-4952-947e-dee559256844",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## calculating mape manually for 10 yr horizon\n",
    "# _10yfc= cross_validation_results[cross_validation_results['sanity check diff']==timedelta(days=3650)]\n",
    "# _10yfc['ape'] = abs(100.*(_10yfc['yhat']-_10yfc['y'])/_10yfc['y'])\n",
    "# _10yfc\n",
    "# _10yfc['ape'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7859faab-a804-40d8-a478-7aabcf6f7452",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "performance_metrics_results_s3 = performance_metrics(cross_validation_results_s3.drop(['real_cutoff', 'sanity check diff'], axis=1, inplace = False))\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(performance_metrics_results_s3)\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(performance_metrics_results_s3[['mape']].style.hide_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e969fbb2-0883-4974-b838-d995af3015b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from prophet.plot import plot_cross_validation_metric\n",
    "fig = plot_cross_validation_metric(cross_validation_results_s3, metric='mape')\n",
    "# fig = plot_cross_validation_metric(cross_validation_results_s1, metric='rmse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab2ff4d-cb5f-46ab-b7ec-4aeb9bebab71",
   "metadata": {},
   "source": [
    "The MAPE score has range from 0 to infinity, and the lower its value the more accurate the predictions are. <br>\n",
    "MAPE = (1 / number of forecasted values) x ∑[( |actual - forecast| ) / |actual| ] x 100 <br>\n",
    "It represents the average of the absolute percentage errors of each entry in a dataset to calculate how accurate the forecasted quantities were in comparison with the actual quantities\n",
    "\n",
    "The RMSE score has range from 0 to  infinity, and the lower its value the more accurate the predictions are. <br>\n",
    "Found by squaring the residuals, finding the average of the residuals, and taking the square root of the result:  sqrt(avg((forecastedval - observedval)^2)) from https://www.statisticshowto.com/probability-and-statistics/regression-analysis/rmse-root-mean-square-error/\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
