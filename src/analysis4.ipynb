{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dbe4f7-dcc3-42b2-b342-0fab4533acb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e434981-2894-4b6c-ab88-e7c2c463554d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os, pathlib, shutil, platform\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "from prophet import Prophet\n",
    "\n",
    "from prophet.diagnostics import cross_validation\n",
    "from prophet.diagnostics import performance_metrics\n",
    "\n",
    "from prophet.plot import plot_plotly, plot_components_plotly\n",
    "from prophet.plot import plot_cross_validation_metric\n",
    "\n",
    "# import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import humanize\n",
    "from datetime import date, datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f925b4-2c6a-4e2c-8d31-93ad7ebcd9b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    " \n",
    "plt.rcParams['figure.figsize']=(20,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890b701c-022e-4b7c-8655-8ab25bac7717",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## rewrote prophet's make future dataframe cuz it doesnt handle very well only monthly data; \n",
    "# fc_model.make_future_dataframe(periods=10, freq='m') ##compare my_make_future_dataframe with the one provided by Prophet\n",
    "def my_make_future_dataframe(df, periods):\n",
    "    last_date=df['ds'].max()\n",
    "    complete_df = df.append(pd.DataFrame([last_date + relativedelta(months = i + 1) for i in range(periods)],\n",
    "                                              columns =['ds']), ignore_index=True, sort=True)\n",
    "    return complete_df\n",
    "\n",
    "def forecast_future(future_samples_count, df, growth = 'linear'):\n",
    "    model = Prophet(growth=growth)\n",
    "    model.fit(df)\n",
    "    \n",
    "    future = my_make_future_dataframe(df, periods=future_samples_count)\n",
    "\n",
    "    forecast = model.predict(future)\n",
    "    return forecast, model\n",
    "\n",
    "def forecast_in_sample(hold_out_samples_count, df, growth = 'linear'):\n",
    "    train_data = df.drop(df.index[-hold_out_samples_count:])\n",
    "    print(train_data.head(4), train_data.tail(4))\n",
    "    print(train_data.shape)\n",
    "    \n",
    "    model = Prophet(growth=growth)\n",
    "    model.fit(train_data)\n",
    "    \n",
    "    future = df[['ds']].reset_index()                         # predicts for all ds values\n",
    "    forecast = model.predict(future)\n",
    "    return forecast, model\n",
    "\n",
    "def forecasted_percentiles(fc_model, input_df, percentiles):     \n",
    "    forecasted_samples = fc_model.predictive_samples(input_df)\n",
    "    forecasted_stats=pd.DataFrame(data=np.transpose(np.percentile(forecasted_samples['yhat'], percentiles, axis=1 )) #made a change, it said 'yhat' before 'Predicted'\n",
    "             ,  columns = ['pct_'+str(x) for x in percentiles])\n",
    "    forecasted_stats.insert(loc=0, column='Predicted', value=input_df['yhat'])\n",
    "    forecasted_stats.insert(loc=0, column='ds', value=input_df['ds'])\n",
    "    return forecasted_stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfb44d0-edb6-45f2-901e-6eeb52cc1b1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preprocessed_data_s1['ds'] = preprocessed_data_s1['ds'].map(str) +\"-01\" + \"-01\"\n",
    "# preprocessed_data_s1['ds'] = pd.to_datetime(preprocessed_data_s1['ds'],format='%Y-%m-%d')\n",
    "# preprocessed_data_s1\n",
    "# preprocessed_data_s1.dtypes\n",
    "# model = Prophet(growth='linear')\n",
    "# model.fit(preprocessed_data_s1)\n",
    "\n",
    "# future_ds = my_make_future_dataframe(preprocessed_data_s1, periods=120)\n",
    "\n",
    "# forecasted_df = model.predict(future_ds)\n",
    "# forecasted_df\n",
    "# #in the 01 and 02 notebooks, we fake the day cuz we have to have a full date, but the month and year are true since the data are monthly aggregations\n",
    "# #here (2 cells above where we change the data type to datetime), we fake the month and the day because we have to have a full date, but the year is true because the data\n",
    "# # are yearly agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd54ac2-fb1d-4b77-b540-2e632ab545f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e761984c-91c1-48ae-a842-bafad54c2c51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_states = ['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', \n",
    "          'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', \n",
    "          'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', \n",
    "          'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming']\n",
    "\n",
    "# use these states for regional nonjet origin these states dest anywhere\n",
    "# my_states = ['Alabama', 'Alaska', 'Arizona', 'California', 'Colorado', 'Florida', 'Georgia', 'Hawaii', 'Idaho', \n",
    "#           'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Michigan', 'Minnesota', 'Montana', \n",
    "#           'Nebraska', 'Nevada', 'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oregon', 'Pennsylvania',\n",
    "#              'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming']\n",
    "\n",
    "# use these states for regional nonjetandjets origin these states dest anywhere; for regional jets origin these states dest anywhere\n",
    "my_states = ['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', \n",
    "          'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', \n",
    "          'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', \n",
    "          'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming']\n",
    "\n",
    "\n",
    "data_path = './../../../data/paav_cargo/agg_data/freight_aggregation__' \n",
    "\n",
    "years_list_a4 = [yr for yr in range (2004,2024)]\n",
    "\n",
    "#abs_yhat is the raw predicted cargo val 10 yrs from now, whereas crt_abs_y is the last recorded raw cargo value (the current y!)\n",
    "\n",
    "def geoplot_cols(future_periods_in_months, future_periods_in_days, your_years_list, origin_data = True, regional_data = True, aircraft_type = \"jets\"):\n",
    "    geoplot_df = pd.DataFrame(columns=['origin', 'dest', 'pct_inc', 'abs_yhat', 'crt_abs_y'])\n",
    "    if origin_data == True:\n",
    "        analysis_type_list = [4]\n",
    "        origin_flag = 'origin'\n",
    "    elif origin_data == False:\n",
    "        analysis_type_list = [5] \n",
    "        origin_flag = 'dest'\n",
    "        \n",
    "        \n",
    "    if regional_data == True:\n",
    "        regional_flag = 'regional'\n",
    "    elif regional_data == False:\n",
    "        regional_flag = 'alldistance'\n",
    "        \n",
    "        \n",
    "    if aircraft_type == 'nonjets':\n",
    "        aircraft_flag = 'nonjets'\n",
    "    if aircraft_type == 'jetsandnonjets':\n",
    "        aircraft_flag = 'jetsandnonjets'\n",
    "    if aircraft_type == \"jets\":\n",
    "        aircraft_flag = 'jets'\n",
    "        \n",
    "     \n",
    "    #concat\n",
    "    full_path_list = [data_path+str(crt_year) + '_analysis'+ str(analysis_type) +'_' + regional_flag +'_'+ aircraft_flag + '.csv' \n",
    "                      for crt_year in your_years_list \n",
    "                      for analysis_type in analysis_type_list]\n",
    "    print(full_path_list[0])\n",
    "    all_datasets= pd.concat([pd.read_csv(str(crt_file_name)) for crt_file_name in full_path_list], keys=your_years_list).reset_index()\n",
    "    \n",
    "    # yearly agg\n",
    "    all_datasets.drop(columns=['level_1'],axis=1,inplace=True)\n",
    "    all_datasets = all_datasets.rename(columns = {'level_0':'year'})\n",
    "    all_datasets = all_datasets.groupby(['year', 'origin', 'dest'], as_index=True, group_keys=True)['y', 'num flights' ].agg(['sum','count'])\n",
    "    all_datasets.reset_index(inplace=True)\n",
    "    all_datasets.columns= ['_'.join(col) for col in all_datasets.columns.values]\n",
    "    all_datasets.drop(['y_count', 'num flights_count','num flights_sum'], axis=1, inplace = True)\n",
    "    all_datasets.rename(columns={'y_sum': 'y', 'ds_':'ds', 'year_':'year', 'origin_':'origin', 'dest_':'dest'}, inplace = True)\n",
    "        \n",
    "    #faking date: only the year aspect of the date is true, the month and day are made up which we do bc prophet requires a full date of this format: yr-mo-day\n",
    "    all_datasets['ds'] = all_datasets['year'].map(str) +\"-01\" + \"-01\"\n",
    "    all_datasets['ds'] = pd.to_datetime(all_datasets['ds'],format='%Y-%m-%d')\n",
    "    \n",
    "   \n",
    "    for crt_state in my_states:\n",
    "        #percent increase calculation\n",
    "        crt_df = all_datasets[all_datasets[origin_flag]==crt_state]\n",
    "        crt_df=crt_df[['y','ds']]\n",
    "        model = Prophet(growth='linear')\n",
    "        model.fit(crt_df)\n",
    "        crt_future_ds = my_make_future_dataframe(crt_df, periods=future_periods_in_months)\n",
    "        crt_forecasted_df = model.predict(crt_future_ds)\n",
    "        crt_last_cargo_value = (crt_df[crt_df['ds'] == crt_df['ds'].max()])['y'].tolist()[0]\n",
    "        #the following tagged code is tagged because it was used back when we had weird states that contained zeroes in their yearly aggregations\n",
    "        # adder=1\n",
    "        # while crt_last_cargo_value==0:\n",
    "        #     crt_last_cargo_value = (crt_df[crt_df['ds'] == crt_df['ds'].iloc[-1-adder]])['y'].tolist()[0]\n",
    "        #     adder = adder + 1\n",
    "        crt_forecasted_df[\"percent increase\"] = (crt_forecasted_df[\"yhat\"]/crt_last_cargo_value -1)*100\n",
    "        crt_last_date=crt_forecasted_df['ds'].max()\n",
    "        crt_focused_df = crt_forecasted_df[crt_forecasted_df['ds']==crt_last_date]\n",
    "        crt_pct_inc = crt_focused_df._get_value(index= (crt_focused_df.index[crt_focused_df['ds'] == crt_focused_df['ds'].max()])[0], \n",
    "                                                col='percent increase')\n",
    "        print(\"The pct inc predicted 10 yrs from now is \" + str(crt_pct_inc) + \" for \" + crt_state)\n",
    "        print(\"The current cargo val is \" + str(crt_last_cargo_value) + \" for \" + crt_state)\n",
    "        \n",
    "        \n",
    "                \n",
    "         #abs cargo value calculation\n",
    "        ## crt_df = all_datasets[all_datasets[origin_flag]==crt_state] ##u alr said all this above! no need to be redundant\n",
    "        ## crt_df=crt_df[['y','ds']]\n",
    "        ## model = Prophet(growth='linear')\n",
    "        ## model.fit(crt_df)\n",
    "        ## crt_future_ds = my_make_future_dataframe(crt_df, periods=future_periods)\n",
    "        ## crt_forecasted_df = model.predict(crt_future_ds)\n",
    "        ## crt_last_date=crt_forecasted_df['ds'].max()\n",
    "        ## crt_focused_df = crt_forecasted_df[crt_forecasted_df['ds']==crt_last_date]\n",
    "        crt_abs_yhat = crt_focused_df._get_value(index= (crt_focused_df.index[crt_focused_df['ds'] == crt_focused_df['ds'].max()])[0], col='yhat')\n",
    "        print(\"The abs cargo val predicted 10 yrs from now is \" + str(crt_abs_yhat) + \" for \" + crt_state)\n",
    "            \n",
    "             \n",
    "        #mape calclation\n",
    "#         crt_m = Prophet()\n",
    "#         crt_m.fit(crt_df) \n",
    "\n",
    "        m_s1 = Prophet()\n",
    "        m_s1.fit(crt_df.drop(crt_df[crt_df['ds'] == pd.datetime(2023, 1, 1)].index,inplace=False)) \n",
    "        initial_time = f'{365*5} days'\n",
    "\n",
    "        crt_cross_validation_results = cross_validation(m_s1, initial=initial_time, period='365 days', horizon = '3650 days', \n",
    "                                                        parallel=\"processes\")\n",
    "        crt_cross_validation_results.rename(columns={'cutoff': 'real_cutoff'},inplace=True) \n",
    "        crt_cross_validation_results['cutoff'] = np.NaN                                     \n",
    "        for index, row in crt_cross_validation_results.iterrows():\n",
    "            crt_cross_validation_results.at[index, 'cutoff'] = row['ds'] - timedelta(days=365*(((index)%10)+1))\n",
    "        crt_performance_metrics_results = performance_metrics(crt_cross_validation_results.drop(['real_cutoff'], axis=1, inplace = False))\n",
    "        crt_mape = (crt_performance_metrics_results[['mape']].iloc[-1])[0]\n",
    "        print(\"The MAPE for 10 yrs from now is \" + str(crt_mape) + \" for \" + crt_state)\n",
    "        \n",
    "        if origin_data == True:\n",
    "            geoplot_df = geoplot_df.append({'origin': crt_state, 'dest': 'all', 'pct_inc':crt_pct_inc, 'abs_yhat':crt_abs_yhat, \"crt_abs_y\":crt_last_cargo_value, \"mape\":crt_mape}, ignore_index=True)\n",
    "        else:\n",
    "            geoplot_df = geoplot_df.append({'origin': 'all', 'dest': crt_state, 'pct_inc':crt_pct_inc, 'abs_yhat':crt_abs_yhat, \"crt_abs_y\":crt_last_cargo_value, \"mape\":crt_mape}, ignore_index=True)\n",
    "\n",
    "    geoplot_df.to_csv('./../../../data/paav_cargo/geoplot_exercise_data/geoplot_data_'+ origin_flag + '_analysis_'+ str(analysis_type_list[0]) + \"_\" + regional_flag +'_'+ aircraft_flag+'.csv', \n",
    "                          mode='a', index=False, header=True)\n",
    "    # return crt_performance_metrics_results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d264d8-6057-4d7d-8d15-94a9723560dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "geoplot_cols(future_periods_in_months=120,future_periods_in_days=3650, your_years_list=years_list_a4, origin_data = True, regional_data = True, aircraft_type = \"jets\")\n",
    "# geoplot_cols(future_periods_in_months=120,future_periods_in_days=3650, your_years_list=years_list_a4, origin_data = True, regional_data = True, aircraft_type = \"jets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad8a11d-2c61-47ff-8548-ebc2c4ecd4d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e970345d-cf15-4e4c-a1fb-c5dde0f68f89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analysis_type_list = [4]\n",
    "years_list_a4 = [yr for yr in range (2004,2024)]\n",
    "\n",
    "full_path_list = [ './../../../data/paav_cargo/agg_data/freight_aggregation__' +str(crt_year) + '_analysis'+ str(analysis_type) +'_' +\n",
    "                  'regional'+'_'+'jets' + '.csv' \n",
    "                      for crt_year in years_list_a4 \n",
    "                      for analysis_type in analysis_type_list]\n",
    "print(full_path_list[0])\n",
    "all_datasets= pd.concat([pd.read_csv(str(crt_file_name)) for crt_file_name in full_path_list], keys=years_list_a4).reset_index()\n",
    "# yearly agg\n",
    "all_datasets.drop(columns=['level_1'],axis=1,inplace=True)\n",
    "all_datasets = all_datasets.rename(columns = {'level_0':'year'})\n",
    "all_datasets = all_datasets.groupby(['year', 'origin', 'dest'], as_index=True, group_keys=True)['y', 'num flights' ].agg(['sum','count'])\n",
    "all_datasets.reset_index(inplace=True)\n",
    "all_datasets.columns= ['_'.join(col) for col in all_datasets.columns.values]\n",
    "all_datasets.drop(['y_count','num flights_sum'], axis=1, inplace = True)\n",
    "all_datasets.rename(columns={'y_sum': 'y', 'ds_':'ds', 'year_':'year', 'origin_':'origin', 'dest_':'dest'}, inplace = True)\n",
    "\n",
    "all_datasets['ds'] = all_datasets['year'].map(str) +\"-01\" + \"-01\"\n",
    "all_datasets['ds'] = pd.to_datetime(all_datasets['ds'],format='%Y-%m-%d')\n",
    "all_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6123a01-0a08-4bf3-94f2-b6c2d8b9e880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_datasets['num flights_count'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45d3f13-22fe-4a3b-b53b-546cc8cb5a9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crt_df = all_datasets[all_datasets['origin']=='Tennessee']\n",
    "crt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2b0ab2-fed0-4b93-bf9d-eaa4d8c62f86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# finding out which states contain any zeroes in their y summations over a year\n",
    "\n",
    "my_states = ['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', \n",
    "          'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', \n",
    "          'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', \n",
    "          'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming']\n",
    "\n",
    "for investigated_crt_state in my_states:\n",
    "    investigated_crt_state_df = all_datasets[all_datasets['origin']==investigated_crt_state]\n",
    "    # if investigated_crt_state_df[investigated_crt_state_df['y']==0.0]:\n",
    "    #     print(\"need to investigate \"+ investigated_crt_state)\n",
    "    for crt_y in investigated_crt_state_df['y']:\n",
    "        if crt_y == 0:\n",
    "            print(\"need to investigate \"+ investigated_crt_state)\n",
    "        \n",
    "list_of_states_to_investigate_origin_reg_nonjets = ['Arkansas', 'Connecticut', 'Delaware', 'Illinois', 'Massachusetts', 'Mississippi', 'Missouri', 'New Hampshire', 'Oklahoma', 'Rhode Island', 'Vermont', 'Virginia']\n",
    "list_of_states_to_investigate_origin_reg_nonjetsandjets = ['Delaware']\n",
    "list_of_states_to_investigate_origin_reg_jets = ['Delaware']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeca25a-33db-4a8d-a357-e2e1084a4e2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crt_df=crt_df[['y','ds']]\n",
    "crt_df\n",
    "crt_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dac3b23-ef7e-40f3-ae0b-fb8866c3ecef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#NEW: remove 2023\n",
    "crt_df = crt_df[crt_df['ds'].year < 2023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b732e3-a89d-46c8-b27f-98f55a190be9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Prophet(growth='linear')\n",
    "model.fit(crt_df)\n",
    "crt_future_ds = my_make_future_dataframe(crt_df, periods=120)\n",
    "crt_future_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f51ced3-a7a8-4a21-b02e-2eeaba258562",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crt_forecasted_df = model.predict(crt_future_ds)\n",
    "crt_forecasted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89db2637-bef6-4e31-b6cd-18ab4a1f5744",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crt_last_cargo_value = (crt_df[crt_df['ds'] == crt_df['ds'].max()])['y'].tolist()[0]\n",
    "crt_last_cargo_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad23583e-bab2-40ae-af14-946d46c4b04e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crt_forecasted_df[\"percent increase\"] = (crt_forecasted_df[\"yhat\"]/crt_last_cargo_value -1)*100\n",
    "crt_forecasted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc9c706-4f2f-4d90-bd8e-3a7f85842996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crt_last_date=crt_forecasted_df['ds'].max()\n",
    "crt_last_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42f08ba-3655-44d6-8fa8-5a4d5efba806",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crt_focused_df = crt_forecasted_df[crt_forecasted_df['ds']==crt_last_date]\n",
    "crt_focused_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b3f3ee-356b-4664-80da-6773dd91b19f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crt_pct_inc = crt_focused_df._get_value(index= (crt_focused_df.index[crt_focused_df['ds'] == crt_focused_df['ds'].max()])[0], col='percent increase')\n",
    "print(\"The pct inc predicted 10 yrs from now is \" + str(crt_pct_inc) + \" for \" + 'Texas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12052c2f-aee0-49e5-85d1-9582d2cc781a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crt_abs_yhat = crt_focused_df._get_value(index= (crt_focused_df.index[crt_focused_df['ds'] == crt_focused_df['ds'].max()])[0], col='yhat')\n",
    "print(\"The abs cargo val predicted 10 yrs from now is \" + str(crt_abs_yhat) + \" for \" + \"Texas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e611ba14-e350-4896-9a19-e131b0e6d633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## mape\n",
    "# initial_time = f'{365*5} days'\n",
    "\n",
    "# crt_cross_validation_results = cross_validation(model, initial=initial_time, period='365 days', horizon = '3650 days', \n",
    "#                                                 parallel=\"processes\")\n",
    "# crt_cross_validation_results.rename(columns={'cutoff': 'real_cutoff'},inplace=True) \n",
    "# crt_cross_validation_results['cutoff'] = np.NaN                                     \n",
    "# for index, row in crt_cross_validation_results.iterrows():\n",
    "#     crt_cross_validation_results.at[index, 'cutoff'] = row['ds'] - timedelta(days=365*(((index)%10)+1))\n",
    "# crt_performance_metrics_results = performance_metrics(crt_cross_validation_results.drop(['real_cutoff'], axis=1, inplace = False))\n",
    "# crt_mape = (crt_performance_metrics_results[['mape']].iloc[-1])[0]\n",
    "# print(\"The MAPE is \" + str(crt_mape) + \" for \" + \"Texas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5310a6b-9cbf-434b-ab85-2b293ce24f2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9392a7-3bfd-4b5b-8f3f-c02dc83353af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m_s1 = Prophet()\n",
    "m_s1.fit(crt_df.drop(crt_df[crt_df['ds'] == pd.datetime(2023, 1, 1)].index,inplace=False))\n",
    "initial_time = f'{365*5} days'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab4c191-497a-4654-a17d-d9f6e70b7e76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crt_cross_validation_results = cross_validation(m_s1, initial=initial_time, period='365 days', horizon = '3650 days', parallel=\"processes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58ab903-afa3-47a7-a6a7-ef7689d98d28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crt_cross_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5f4240-7af6-4e2a-a021-5bc65b9bd95f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crt_cross_validation_results.rename(columns={'cutoff': 'real_cutoff'},inplace=True)\n",
    "crt_cross_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2b2b56-67ba-4567-b6f4-1028aba0788f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crt_cross_validation_results['cutoff'] = np.NaN \n",
    "crt_cross_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50740d45-8303-4b3a-b99c-a7ed4a5d2c7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for index, row in crt_cross_validation_results.iterrows():\n",
    "    crt_cross_validation_results.at[index, 'cutoff'] = row['ds'] - timedelta(days=365*(((index)%10)+1))\n",
    "crt_cross_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20e9a56-3405-4fbd-94b5-1ad2cebc5e9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crt_performance_metrics_results = performance_metrics(crt_cross_validation_results.drop(['real_cutoff'], axis=1, inplace = False))\n",
    "crt_performance_metrics_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd25457-6ddc-4b8f-99f6-15d9499418d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crt_mape = (crt_performance_metrics_results[['mape']].iloc[-1])[0]\n",
    "print(\"The MAPE is \" + str(crt_mape) + \" for \" + \"Texas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdb9587-72ef-461c-914d-a048cf87ff33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca2a9d5-4aee-464d-851b-b00eb1da48e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e5eb20-823e-4279-be21-4f6b4905faf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4458c64e-d60b-403b-8505-f1f47e267c28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4706f9bf-4b97-4589-93c5-abe40abf56cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40644813-971d-4a05-9cbb-fdef44846cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b68a58-095f-43cb-880e-9ffb5a973586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5289b603-b707-4d74-80bb-a2306ead3a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d379f309-a825-4c5f-b770-eda228beb3d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5131b7b-8d51-478b-8e88-968fa0e4cf7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bb0b18-87ae-4df2-8d29-ae6f1cd06670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788f3f7f-cac3-4fce-9b1f-f06fff1c69b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1698ab0d-4e73-4039-885c-72f469a40b73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d2cbef-6ab4-4ed9-a305-a07cbfa3a64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babb25f6-bd1b-4867-a4da-81f1586fcdcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f93163-0461-4c4e-b22f-eb6c23e98cec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d28aa44-bd52-48a5-b46c-cf3be0df6225",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_datasets_v2(data_path, years_list, analysis_type_list):\n",
    "    \"\"\"\n",
    "    Concatanates a list of datasets into one dataframe, and also does yearly agg\n",
    "\n",
    "    Returns:\n",
    "    Concatanated list as a dataframe\n",
    "    \"\"\"\n",
    "    #concat\n",
    "    full_path_list = [data_path+str(crt_year) + '_analysis'+ str(analysis_type) +'_' + str(regional_flag)+'_'+str(non_jets_flag) + '.csv' \n",
    "                      for crt_year in years_list \n",
    "                      for analysis_type in analysis_type_list]\n",
    "    print(full_path_list[0])\n",
    "    all_datasets= pd.concat([pd.read_csv(str(crt_file_name)) for crt_file_name in full_path_list], keys=years_list).reset_index()\n",
    "    \n",
    "    # yearly agg\n",
    "    all_datasets.drop(columns=['level_1'],axis=1,inplace=True)\n",
    "    all_datasets = all_datasets.rename(columns = {'level_0':'year'})\n",
    "    all_datasets = all_datasets.groupby(['year', 'origin', 'dest'], as_index=True, group_keys=True)['y', 'num flights' ].agg(['sum','count'])\n",
    "    all_datasets.reset_index(inplace=True)\n",
    "    all_datasets.columns= ['_'.join(col) for col in all_datasets.columns.values]\n",
    "    all_datasets.drop(['y_count', 'num flights_count','num flights_sum'], axis=1, inplace = True)\n",
    "    all_datasets.rename(columns={'y_sum': 'y', 'ds_':'ds', 'year_':'year', 'origin_':'origin', 'dest_':'dest'}, inplace = True)\n",
    "\n",
    "# my_testing[\"year\"] = my_testing['ds'].dt.year #tagged this cuz level_0 column takes care of it\n",
    "\n",
    "    return all_datasets    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684c8aca-60b5-45e3-9021-2ff7ef14657d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "years_list_a4 = [yr for yr in range (2004,2024)]\n",
    "data_path='./../../../data/paav_cargo/agg_data/freight_aggregation__' \n",
    "analysis_type_list_a4 = [4]\n",
    "\n",
    "regional_flag = 'regional'\n",
    "non_jets_flag = 'nonjets'\n",
    "origindater = load_datasets_v2(data_path, years_list_a4,analysis_type_list_a4)\n",
    "origindater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c4eca7-dbf7-4350-9ca8-30bf17d969dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crt_df = origindater[origindater['origin']==\"Rhode Island\"]\n",
    "crt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a98ae7-54fa-416e-9c40-16f2917a9457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crt_df['ds'] = crt_df['year'].map(str) +\"-01\" + \"-01\"\n",
    "crt_df['ds'] = pd.to_datetime(crt_df['ds'],format='%Y-%m-%d')\n",
    "crt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a19098e-080e-44c3-b8fc-39e840549c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "crt_df=crt_df[['y','ds']]\n",
    "crt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f3a89d-e0f2-49ca-9ae1-66d28d97d3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Prophet(growth='linear')\n",
    "model.fit(crt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61840cd0-617b-409b-925e-d6320be30ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_ds = my_make_future_dataframe(crt_df, periods=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cf6427-097c-4e6e-9ee4-c87f49e1c284",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_ds.head(3)\n",
    "future_ds.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e842b-a994-49b1-b072-b5594cb350c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasted_df = model.predict(future_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0a13ae-fcd8-44b8-87f2-0b4901874a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasted_df.head(3)\n",
    "forecasted_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ea823c-cc6d-41be-a71e-b8638c090546",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_cargo_value = (crt_df[crt_df['ds'] == crt_df['ds'].max()])['y'].tolist()[0]\n",
    "forecasted_df[\"percent increase\"] = (forecasted_df[\"yhat\"]/last_cargo_value -1)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f76c53c-f901-4911-8023-a91ef382e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "crt_df\n",
    "last_cargo_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092c7912-a28e-4c30-9d80-541fddb87bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasted_df.head(3)\n",
    "forecasted_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d229668-b120-450a-8893-ed952bd063fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date=forecasted_df['ds'].max()\n",
    "focused_df = forecasted_df[forecasted_df['ds']==last_date]\n",
    "focused_df\n",
    "my_var = focused_df._get_value(index=135,col='percent increase')\n",
    "my_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032c02bb-1a59-4ebd-941b-05e1343106d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=(focused_df.index[focused_df['ds'] == focused_df['ds'].max()])[0]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12881f96-1484-4f77-b641-5904eb1b6d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e3fa19-25d4-4c4c-84be-fb97b878683e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9942086-b14e-4580-89a2-56fab3c2bc24",
   "metadata": {},
   "source": [
    "# Problem states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2dfc6c-6337-4759-ac76-fcf5a1f2928c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for crt_weird_state in list_of_states_to_investigate:\n",
    "    all_datasets[all_datasets['origin']==crt_weird_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da614c0-4b81-44c7-82d9-8ffa0fefd3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_datasets(data_path, years_list, analysis_type_list):\n",
    "#     \"\"\"\n",
    "#     Concatanates a list of datasets into one dataframe, and also does yearly agg\n",
    "\n",
    "#     Returns:\n",
    "#     Concatanated list as a dataframe\n",
    "#     \"\"\"\n",
    "#     #concat\n",
    "#     full_path_list = [data_path+str(crt_year) + '_analysis'+ str(analysis_type) +'_' + str(regional_flag)+'_'+str(non_jets_flag) + '.csv' \n",
    "#                       for crt_year in years_list \n",
    "#                       for analysis_type in analysis_type_list]\n",
    "#     print(full_path_list[0])\n",
    "#     all_datasets= pd.concat([pd.read_csv(str(crt_file_name)) for crt_file_name in full_path_list], keys=years_list).reset_index()\n",
    "    \n",
    "#     #yearly agg\n",
    "#     all_datasets.drop(columns=['level_1'],axis=1,inplace=True)\n",
    "#     all_datasets = all_datasets.rename(columns = {'level_0':'year'})\n",
    "#     all_datasets = all_datasets.groupby(['year', 'origin', 'dest'], as_index=True, group_keys=True)['y', 'num flights' ].agg(['sum','count'])\n",
    "#     all_datasets.reset_index(inplace=True)\n",
    "#     all_datasets.columns= ['_'.join(col) for col in all_datasets.columns.values]\n",
    "#     all_datasets.drop(['origin_', 'dest_', 'y_count', 'num flights_count','num flights_sum'], axis=1, inplace = True)\n",
    "#     all_datasets.rename(columns={'y_sum': 'y', 'year_':'ds' }, inplace = True)\n",
    "\n",
    "# # my_testing[\"year\"] = my_testing['ds'].dt.year #tagged this cuz level_0 column takes care of it\n",
    "\n",
    "#     return all_datasets    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# years_list = [yr for yr in range (2002,2024)]\n",
    "# years_list_s1 = [yr for yr in range (2004,2024)]\n",
    "# data_path='./../../../data/paav_cargo/agg_data/freight_aggregation__' \n",
    "\n",
    "# analysis_type_list = [analysis_type for analysis_type in range(11)]\n",
    "# # regional_flag = 'alldistance' # , 'regional']\n",
    "# # non_jets_flag = 'jetsandnonjets' # 'nonjets']\n",
    "# regional_flag = 'regional' # , 'alldistance']\n",
    "# non_jets_flag = 'nonjets' # 'jetsandnonjets']\n",
    "\n",
    "\n",
    "# # analysis_type_list = [4]\n",
    "# # preprocessed_data_s1_breakdown = load_datasets(data_path, years_list_s1, analysis_type_list)\n",
    "\n",
    "# analysis_type_list = [0]\n",
    "# preprocessed_data_s1 = load_datasets(data_path, years_list_s1, analysis_type_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# percentiles = [2.5, 97.5]\n",
    "# forecasted_stats5 = forecasted_percentiles(m_s5, cross_validation_results_s5, percentiles = percentiles)\n",
    "# forecasted_stats5\n",
    "# forecasted_stats5[\"percent increase\"] = (forecasted_stats5[\"Predicted\"]/last_cargo_value -1)*100\n",
    "# forecasted_stats5['Predicted'] = forecasted_stats5['Predicted'].apply(humanize.intword)\n",
    "# forecasted_stats5['pct_2.5'] = forecasted_stats5['pct_2.5'].apply(humanize.intword)\n",
    "# forecasted_stats5['pct_97.5'] = forecasted_stats5['pct_97.5'].apply(humanize.intword)\n",
    "# forecasted_stats5.rename(columns={'pct_2.5': 'lower bound of 95% CI', 'pct_97.5': 'upper bound of 95% CI', 'Predicted':'predicted'}, inplace = True)\n",
    "# forecasted_stats5['origin'] = origin_place\n",
    "# forecasted_stats5['dest'] = dest_place\n",
    "# forecasted_stats5['date these results were obtained'] = str(date.today())\n",
    "# data_types_dict = {'ds': str}\n",
    "# forecasted_stats5 = forecasted_stats5.astype(data_types_dict)\n",
    "# forecasted_stats5\n",
    "# print(\"Today's date:\", today)\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# df1 = forecasted_stats.loc[(pd.to_datetime(forecasted_stats['ds']) == pd.to_datetime('2024-03-01')) ]\n",
    "# df2 = forecasted_stats.loc[(pd.to_datetime(forecasted_stats['ds']) == pd.to_datetime('2026-03-01')) ]\n",
    "# df3 = forecasted_stats.loc[(pd.to_datetime(forecasted_stats['ds']) == pd.to_datetime('2030-03-01')) ]\n",
    "# df1\n",
    "# df2\n",
    "# df3\n",
    "\n",
    "\n",
    "# def forecasted_percentiles(fc_model, input_df, percentiles):     \n",
    "#     forecasted_samples = fc_model.predictive_samples(input_df)\n",
    "#     forecasted_stats=pd.DataFrame(data=np.transpose(np.percentile(forecasted_samples['yhat'], percentiles, axis=1 ))\n",
    "#              ,  columns = ['pct_'+str(x) for x in percentiles])\n",
    "#     forecasted_stats.insert(loc=0, column='Predicted', value=input_df['yhat'])\n",
    "#     forecasted_stats.insert(loc=0, column='ds', value=input_df['ds'])\n",
    "#     return forecasted_stats \n",
    "\n",
    "\n",
    "\n",
    "# def forecast_future(future_samples_count, df, growth = 'linear'):\n",
    "#     model = Prophet(growth=growth)\n",
    "#     model.fit(df)\n",
    "    \n",
    "#     future = my_make_future_dataframe(df, periods=future_samples_count)\n",
    "\n",
    "#     forecast = model.predict(future)\n",
    "#     return forecast, model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
